---
title: "Vpliv kršenja predpostavk linearne regresije na njene rezultate"
subtitle: "Seminarska naloga pri predmetu Računsko zahtevne metode"
author: "Anja Žavbi Kunaver in Vesna Zupanc"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r}
# Potrebni paketi:
library(ggplot2) # za risanje grafov
library(knitr) # lepši prikaz tabel
library(gridExtra) # lepši prikaz tabel
library(grid)
library(foreign)
library(MASS)
library(lattice)
library(multiUS) # npr. za izpis p-vrednosti
library(rms)
library(tidyverse)
library(reshape2)
library(psych)
library(Hmisc)
library(rgl)
library(blockmodeling)
library(mclust)
library(sn)
library(corrplot)
```

\pagebreak

# Uvod

Obravnavana metoda je linearna regresija in zanima nas, kako kršenje predpostavk (konkretneje nenormalna porazdelitev ostankov in močna korelacija med pojasnjevalnimi spremenljivkami) vpliva na njene rezultate.
Preverjali bomo vpliv različnih dejavnikov na pristranost in širino intervalov zaupanja regresijskih koeficientov.
Ti dejavniki so velikost vzorca, moč korelacije med pojasnjevalnimi spremenljivkami, asimetrija porazdelitve ostankov, asimetrija porazdelitve pojasnjevalnih spremenljivk ter število vključenih spremenljivk v modelu.
Poleg tega bomo preverjali, če probleme kršenja predpostavk lahko (vsaj delno) odpravimo z uporabo posplošenih linearnih modelov.

# Teoretični del

Naloga je osredotočena na linearno regresijo, ki spada pod posplošene linearne modele. V tem poglavju so najprej bolj splošno predstavljeni posplošeni linearni modeli, nato pa podrobneje model linearne regresije in metode, s pomočjo katerih lahko ocenjujemo regresijske koeficiente.

## Posplošeni linearni modeli

Posplošeni linearni mešani model izrazimo kot $$Y=X\beta+Z\alpha+\epsilon,$$ kjer je $Y$ opazovani slučajni vektor, $X$ matrika znanih vrednosti pojasnjevalnih spremenljivk, $\beta$ neznan vektor regresijskih koeficientov (fiksni učinki), $Z$ znana matrika, $\alpha$ vektor naključnih učinkov in $\epsilon$ vektor napak. $\alpha$ in $\epsilon$ sta neopazovana. Predpostavimo, da sta nekorelirana.

V matrični obliki model izgleda takole: $$
\begin{bmatrix}
Y_{1} \\
Y_{2} \\
\vdots \\
Y_{n}
\end{bmatrix}
=
\begin{bmatrix}
X_{1,1} & X_{1,2} & \dots & X_{1,p} \\
X_{2,1} & X_{2,2} & \dots & X_{2,p} \\
\vdots & \vdots &  & \vdots \\
X_{n,1} & 0 & \dots & X_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{p}
\end{bmatrix}
+
\begin{bmatrix}
Z_{1,1} & Z_{1,2} & \dots & Z_{1,q} \\
Z_{2,1} & Z_{2,2} & \dots & Z_{2,q} \\
\vdots & \vdots &  & \vdots \\
Z_{n,1} & Z_{n,2} & \dots & Z_{n,q}
\end{bmatrix}
\begin{bmatrix}
\alpha_{1} \\
\alpha_{2} \\
\vdots \\
\alpha_{q}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{n}
\end{bmatrix}
.$$

Linearni mešani modeli se delijo na Gaussove ali normalne in ne-Gaussove.
Pomembna predpostavka pri normalnih linearnih mešanih modelih je normalna porazdeljenost vektorja slučajnih učinkov $\alpha \sim N(0,\sigma^2 I_q)$ in vektorja slučajnih odstopanj $\epsilon \sim N(0,\tau^2 I_n)$, ki nista nujno enakih razsežnosti. Druga pomembna predpostavka je neodvisnost slučajnih vektorjev $\alpha$ in $\epsilon$. Prednost uporabe nenormalnih linearnih mešanih modelov pred normalnimi je v tem, da so bolj fleksibilni za modeliranje
(Maver, 2018, str. 6).

## Linearna regresija

Linearna regresija je statistični model, ki ga v najbolj enostavni obliki lahko zapišemo kot:
$$Y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i},$$ kjer so $\epsilon_{i}$ med seboj neodvisne slučajne spremenljivke, $x_{i}$ pa dane vrednosti.
Velja $\epsilon_{i} \sim N(0, \sigma^{2})$ za vsak $i$ in tako $Y_{i} \sim N(\beta_{0} + \beta_{1}x_{i},\ \sigma^{2})$. Model lahko razširimo na več linearnih parametrov: $$Y_{i} = \beta_{0} + \beta_{1} x_{i1} + \dots +\beta_{p} x_{ip} + \epsilon_{i},$$ kjer so $\epsilon_{i}$ neodvisne enako porazdeljene slučajne spremenljivke, za $1\leq i\leq n$.

Lahko ga zapišemo tudi v matrični obliki: $$Y = X \beta + \epsilon.$$

Med temeljne predpostavke regresijskega modela spada predpostavka, da med neodvisnimi spremenljivkami ni popolne kolinearnosti ali multikolinearnosti. Najbolj tipičen vzrok za kršenje te predpostavke je, da smo v model kot neodvisni vključili dve spremenljivki, med katerima obstaja močna linearna povezanost. Do multikolinearnosti pride tudi, če v model vključimo več spremenljivk kot je velikost vzorca. Na multikolinearnost posumimo, če se v modelu determinacijski koeficient izkaže za statistično značilnega, od regresijskih koeficientov pa nobeden.

Opazovanja so med seboj neodvisna. V primeru kršenja te predpostavke je smiselno uporabiti posplošene linearne modele, običajno longitudinalni (vzdolžni) model.
Vse predpostavke linearnega regresijskega modela so navedene v naslednjem razdelku.

## Metoda najmanjših kvadratov (MNK)
Pri 16 letih jo je odkril nemški matematik Carl F. Gauss. Zaradi svojih lastnosti je najbolj razširjena metoda ocenjevanja regresijskih koeficientov
(Pfajfar, 2018, str.53).

Pri MNK na primeru osnovnega regresijskega modela velikosti $p=1$ iščemo $\beta_{0}$ in $\beta{1}$ tako, da bo vsota kvadratov ostankov najmanjša možna. Pri danih $(x_{i}, y_{i})$ torej iščemo $$\min_{\beta_{0}, \beta_{1}} \sum_{i=1}^{n} (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}.$$

Za razumevanje oznak v predpostavkah metode ločimo dva modela, in sicer linearni vzorčni regresijski model $y_i=b_1+b_2x_i+e_i$ in linearni populacijski regresijski model $y=\beta_1+\beta_2x_i+u_i$.
Pfajfar (2018) navaja naslednje predpostavke metode najmanjših kvadratov:

- linearnost regresijskega modela: $y=\beta_1+\beta_2x_i+u_i$

- ničelna povprečna vrednost $u_i$: $E(u_i)=0$

- homoskedastičnost: $Var(u_i)=E(u_i^2)=\sigma^2$

- odsotnost avtokorelacije: $cov(e_i, e_j|x_i, x_j)=0$ za vsak $i\ne j$

- nekoreliranost med pojasnjevalnimi spremenljivkami in slučajno spremenljivko $u$: $Cov(x_2,u)=Cov(x_3,u)=...=Cov(x_k,u)=0$

- število opazovanj mora presegati število ocenjenih parametrov oz. pojasnjevalnih spremenljivk: $n>k$

- $Var(X)$ je končno pozitivno število

- pravilno specificiran regresijski model: vključene vse relevantne pojasnjevalne spremenljivke in izbrana ustrezna funkcijska oblika modela

- odsotnost popolne multikolinearnosti: $\lambda_1X_1+\lambda_2X_2+...+\lambda_kX_k=0$

- slučajna spremenljivka $u$ je normalno porazdeljena: $u_i \sim N(0,\sigma_u^2)$.
Posledično je pogojna porazdelitev odvisne spremenljivke $y$ tudi normalna in sicer $N(\beta_1x_{1i}+...+\beta_kx_{ki},\sigma_u^2)$

## Metoda iterativnega uteženega povprečja najmanjših kvadratov (IWLS)

Naj bo $Y$ vektor meritev in $X$ matrika znanih konstant. Naj bo $E(Y) = X\beta$, kjer $\beta$ kot do sedaj predstavlja vektor neznanih regresijskih koeficientov. Cenilko za $\beta$ se po uteženi metodi najmanjših kvadratov dobi z minimizacijo izraza 
\begin{equation}
(Y - X\beta)'W(Y - X\beta),
\end{equation}
kjer je $W$ znana simetrična matrika uteži.

Brez škode za splošnost naj bo rang matrike $X$ poln in naj velja rang$X=p$.
Potem je za vsako nesingularno (simetrično) matriko $W$ minimum izraza (1) enak 
\begin{equation}
\hat{\beta}_W = (X'WX)^{-1}X'WY.
\end{equation}
Cenilko za $\beta$ po običajni metodi najmanjših kvadratov (ang. ordinary least squares, OLS) se dobi kot poseben primer, za $W=I$:
\begin{equation}
\hat{\beta}_I=(X'X)^{-1}X'Y .
\end{equation}

Izkaže se, da je v smislu čim manjše variance optimalna izbira za matriko $W$ matrika  $W = V^{-1},$ kjer je $V = Var(Y)$. Tako dobljena cenilka za parameter $\beta$ je najboljša, saj je z njeno uporabo dosežena najmanjša možna variabilnost med vsemi drugimi alternativami. V tem primeru se dobljeni cenilki za $\beta$ reče najboljša linearna nepristranska cenilka ali $BLUE$ (ang. best linear unbiased estimator):
\begin{equation}
\hat{\beta}_{BLUE} = (X'V^{-1}X)^{-1}X'V^{-1}Y.
\end{equation}

V enačbi za $\beta_{BLUE}$ nastopa tudi $V$, ki pa tipično ni znana. Zaradi poenostavitve je v nadaljevanju prikazan postopek izračuna cenilke $BLUE$ zgolj na uravnoteženem primeru.
Naj bo $Y_{ij}, j = 1,..., \tilde{m}$, vektor meritev na $i-$tem posamezniku, kjer je $\tilde{m}$ fiksno število. V uravnoteženem primeru so na vseh posameznikih meritve pridobljene ob določenih časovnih trenutkih $t_1, . . . , t_{\tilde{m}}$ . Za $i-$tega posameznika se lahko vektor
meritev zapiše kot $Y_i = (Y_{ij})_{\leq j\leq \tilde{m}}, i = 1,...,n$. Naj bodo $Y_1, . . . , Y_n$ med seboj neodvisni in naj za njih velja $E(Y_i) = X_i\beta$ in $Var(Y_i) = V_0$. Tu je $X_i$ matrika znanih konstant in $V_0 = (v_{qr})_{1\leq q, r\leq \tilde{m}}$ neznana variančno kovariančna matrika. Iz tega sledi, da je $V = diag(V_0, . . . , V_0)$. Ker je število meritev $\tilde{m}$ na vsakem posamezniku fiksno, je mogoče poiskati dosledno cenilko za $V$. Če bi bil parameter $\beta$ znan, bi bila dosledna cenilka za $V$ kar $$\hat{V} = diag(\hat{V}_0,...,\hat{V}_0),$$
kjer je
\begin{equation}
\hat{V}_0 =\frac{1}{n}\sum^n_{i=1} (Y_i - X_i\beta)(Y_i - X_i\beta)'.
\end{equation}

Če bi bila $V$ znana, bi lahko za izračun najboljše linearne nepristranske cenilke za $\beta$ uporabili (4), če pa bi poznali $\beta$, bi z (5) dobili dosledno cenilko za $V$.

Metodi, kjer ni treba poznati ne $\beta$, ne $V$, pa se reče iterativno uteženo povprečje najmanjših kvadratov (ang. iterative weighted least squares, IWLS). Postopek omenjene metode je sledeč:

* Najprej se izračuna cenilka za $\beta$ po običajni metodi najmanjših kvadratov s pomočjo (3).
* Nato se izračuna $\hat{V}$ po (5), kjer je $\beta$ zamenjan z $\hat{\beta}_I$ izračunanim en korak prej.
* V zadnjem koraku pa se na desni strani (4) matriko $V$ zamenja z njeno cenilko $\hat{V}$, izračunano na prejšnjem koraku.

Na tak način se dobi cenilka za $\beta$ po prvi iteraciji, nato pa se postopek ponavlja.
Pod predpostavko normalnosti se izkaže, da če IWLS konvergira, bo cenilka v limiti enaka cenilki, dobljeni po metodi največjega verjetja (celotno podpoglavje je povzeto po Maver, 2018, strani 19-21).


# Generiranje podatkov

## Parametri

Fiksni parametri pri generiranju podatkov so sledeči:

- formula za generiranje podatkov:

$$y_i = 1 + x_1 + x_2 + 0x_3 + \epsilon_i .$$

Pri generiranju podatkov bomo spreminjali sledeče:

- velikost vzorca $n \in \{10, 50, 100, 500, 1000\}$;

- korelacija med pojasnjevalnimi spremenljivkami ($cor \in \{0, 0.3, 0.6, 0.9\}$);

- porazdelitev pojasnjevalnih spremenljivk:
  $X_j \sim Gamma(\delta,5)$, $j=1,2,3$, $\delta=2,5$;

- porazdelitev napak ($Gamma(\alpha, 5)$), kjer bomo parameter $\alpha$ spreminjali tako, da dobimo različno močno asimetrične porazdelitve ($\alpha \in \{1, 3, 5\}$;

- v modelu ne upoštevamo vseh neodvisnih spremenljivk (spreminjamo število spremenljivk, ki jih upoštevamo): enkrat vključimo vse spremenljivke, enkrat izločimo $X_3$ (ki nima vpliva na odzivno spremenljivko), enkrat pa izločimo $X_2$.

Pri generiranju koreliranih gama spremenljivk lahko uporabimo sledečo lastnost: Če $X_i \sim Gamma(k_i, \theta)$, potem je
$$\sum_{i=1}^{n} X_i \sim Gamma(\sum_{i=1}^{n} k_i, \theta).$$

Ker zaradi spreminjanja parametrov v gama porazdelitvi pri porazdelitvi napak ne spreminjamo samo asimetričnosti ampak tudi varianco in ker vemo, da ima različna varianca vpliv na model, smo spremenljivke napak skalirali. Napake smo skalirali tako, da smo vrednosti delili s teoretično standardno napako in tako poskrbeli, da imajo vse porazdelitve napak enako varianco. 

Pri pregledu literature sva ugotovili, da za generiranje odvisnih gama spremenljivk lahko uporabimo kar funkcijo \emph{rmvgamma()} in si s tem olajšamo delo pri generiranju podatkov. Funkcija sprejme naslednje parametre: $n$ (število vektorjev, ki jih želimo generirati), $corr$ (korelacijska matrika) ter parametra `shape` in `rate`, ki sta privzeto nastavljena na 1. \textcolor{red}{Manjka še opis, kako funkcija generira vrednosti.}

## Funkciji lm() in glm()

Funkcija \emph{lm()} se uporablja za ocenjevanje linearnih modelov. Avtomatično uporablja osnovno metodo najmanjših kvadratov, lahko pa nastavimo tudi na metodo uteženih najmanjših kvadratov
(lm iz RDocumentation, 2020). V tej seminarski nalogi uporabljamo samo osnovno metodo najmanjših kvadratov.

V linearnem modelu $Y_i = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip} + \epsilon_{i}$ velja
$E(Y_i) = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip}$. Slednjo enačbo lahko z uporabo primerno definirane funkcije $g$ posplošimo do $$g(E(Y_i)) = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip}.$$ Tu z indeksom $i$ označujemo i-tega posameznika. Prejšnja enačba je poseben primer, kjer je $g$ identiteta.
Z uporabo funkcije \emph{glm()} in znotraj primerno definirane funkcije $g$ lahko generiramo več posplošenih linearnih modelov. Prednost te funkcije je tudi v tem, da lahko poleg normalne porazdelitve nastavimo še katero drugo porazdelitev ostankov. To naredimo tako, da npr. v primeru gama porazdelitve ostankov znotraj funkcije \emph{glm()} definiramo \emph{family=Gamma(link="identity")}. Tu \emph{identity} pomeni, da za funkcijo \emph{g} vzamemo kar identiteto. Funkcija parametre modela ocenjuje po metodi iterativnega uteženega povprečja najmanjših kvadratov
(glm iz RDocumentation, 2020).

## Ocenjevanje intervalov zaupanja

Za ocenjevanje intervalov zaupanja pri obeh metodah uporabimo funkcijo \emph{confint.default}, ki računa intervale zaupanja na podlagi standrdnih napak. Najprej smo poskusili z uporabo navadne funkcije \emph{confint}, vendar je prihajalo do problemov pri posplošenih linearnih modelih. Slednja funkcija predpostavlja normalnost, funkcija \emph{confint.default} pa temelji na asimptotski normalnosti (confint iz RDocumentation, 2020). Za stopnjo zaupanja vzamemo $\alpha=0.05$.


## Pričakovanja

Pri večji korelaciji med pojasnjevalnimi spremenljivkami pričakujemo širše intervale zaupanja regresijskih koeficientov ne glede na izbiro metode. Zaradi večje širine intervalov zaupanja ne pričakujemo večjih sprememb v pokritosti.

Večje razlike med metodami pričakujemo predvsem pri manjših velikostih vzorcev in večji asimetriji porazdelitve ostankov. Pri dovolj velikih vzorcih pričakujemo podobne rezultate obeh metod, prav tako pa seveda tudi manjšo variabilnost rezultatov.

Pričakujemo, da lahko kršenje predpostavke o normalni porazdeljenosti ostankov rešimo z uporabo posplošenih linearnih modelov z ustrezno definirano porazdelitvijo ostankov oz. odzivne spremenljivke. Pričakujemo, da bolj kot bo porazdelitev ostankov asimetrična (manjša vrednost parametra$\alpha$), slabši bodo rezultati funkcije \emph{lm()} in posledično večje razlike med rezultati funkcij \emph{lm()} in \emph{glm()}.

V primeru, ko iz modela izločimo spremenljivko $X_3$, ne pričakujemo posebnih sprememb v rezultatih, saj spremenljivka nima vpliva na vrednost pojasnjevalne spremenljivke. V primeru, ko izločimo spremenljivko $X_2$, pa pričakujemo spremembe v rezultatih - širše intervale zaupanja regresijskih koeficientov in slabšo pokritost.

Porazdelitev pojasnjevalnih spremenljivk preverimo za dve porazdelitvi - $gama(2,5)$, ki je precej asimetrična in $gama(5,5)$, ki je zelo podobna normalni porazdelitvi. Zanima nas, če in kako asimetrija pojasnjevalnih spremenljivk vpliva na ocene regresijskih koeficientov. Pričakujemo, da bo v primeru asimetrične porazdelitve prišlo do manjše pokritosti in večje širine intervalov zaupanja.

```{r fig.cap="Različni porazdelitvi gama"}
par(mfrow=c(1,2))

x <- seq(0,5,.001)
y <- dgamma(x,1,5)
plot(x,y, main="gamma(2,5)")

y <- dgamma(x,5,5)
plot(x,y, main="gamma(5,5)")
```

# Predstavitev rezultatov

```{r}
# potrebni paketi
library("MASS")
library("sjstats")
library("doRNG")
library("parallel")
library("doParallel")
library("reshape2")
library("ggplot2")
library("see")
library("effectsize")
library("lcmix")
library("dplyr")

source("rezultati_simulacije.R") # kasneje uporabi load (ko bodo simulacije shranjene s funkcijo "save")
simulacija = as.data.frame(simulacija)
```

Rezultati se nanašajo na veliko parametrov in za več različnih načinov modeliranja, zato si bomo rezultate ogledali na različne načine.
Ker imata koeficienta $X_1$ in $X_2$ enak vpliv na odzivno spremenljivko, $X_3$ pa nima vpliva, se lahko osredotočimo le na koeficient $\beta_1$.

## Polni model

### Asimetrična porazdelitev pojasnjevalnih spremenljivk

Za začetek si poglejmo rezultate oz. izbrane mere na polnem modelu, kjer so upoštevane vse spremenljivke in pri asimetrični porazdelitvi spremenljivke $X$. Naslednje dva grafa tako prikazujeta pokritost in širine intervalov zaupanja za koeficient $\beta_1$ za posamezen model (`gls` ali `ols`) glede na velikost vzorca in korelacijo. 

```{r}
pokritost <- c("pokritost_B0", "pokritost_B1", "pokritost_B2", "pokritost_B3")
sirina <- c()
data_pokritost <- melt(simulacija, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = pokritost)
data_pokritost$korelacija = data_pokritost$korelacija %>% round(1)
sirina <- c("sirina_B0", "sirina_B1", "sirina_B2", "sirina_B3")
data_sirina <- melt(simulacija, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = sirina)
data_sirina$korelacija = data_sirina$korelacija %>% round(1)
```


```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```

Iz slik lahko vidimo, da imata precej očiten vpliv pri obeh metodah (gls in ols) korelacija in velikost vzorca. Z večanjem korelacije se sama pokritost ne spremeni bistveno, se pa močno povečajo intervali zaupanja, medtem ko z večanjem velikosti vzorca pridobimo večjo pokritost in ožje intervale zaupanja. Pri stopnji asimetrije ostankov pa ni opaziti bistvenega vpliva na rezultate. 

Razberemo tudi, da ima model glm nekoliko boljšo pokritost, razlika pa je manjša pri manjših vzorcih. Velja pa tudi, da pri glm modelu dobimo nekoliko širše intervale zaupanja, od koder najverjetneje izhaja tudi boljša pokritost. Intervali zaupanja pri metodi ols so torej preozki, posledično pa interval zaupanja za koeficient večkrat ne vsebuje prave vrednosti koeficienta. Razlike v pokritosti sicer niso velike, vseeno pa so prisotne. Rezultati so za manjše število izbranih parametrov n in korelacije prikazani tudi v spodnjih dveh tabelah, kjer bomo lažje razbrali razlike. Prikazane razlike so v obeh primerih razlike glm glede na ols metodo, v tabeli s širino intervalov zaupanja so prikazane razlike še v deležu glede na širino intervala zaupanja pri metodi ols. 


```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Velja torej, da ima v povprečju glm metoda od 1 do 2 o.t. boljšo pokritost, vendar pa ima tudi do $14\%$ širše intervale zaupanja. Ta razlika med intervali zaupanja se najbolj poveča z večanjem velikosti vzorca. V splošnem pa velja, da pokritost z večanjem velikosti vzorca zraste za $4\%$. Visoka korelacija v kombinaciji z majhnim vzorcem je najbolj problematična, tam imamo tudi manjšo pokritost in precej široke intervale zaupanja. 


Prejšnji komentar: Na Sliki 1 vidimo, da so pokritosti intervalov zaupanja za regresijske koeficiente $\beta_1$, $\beta_2$ in $\beta_3$ vedno blizu 100%, ne glede na uporabljeno metodo, velikost vzorca, korelacijo med pojasnjevalnimi spremenljivkami in stopnjo asimetrije ostankov.
Močno izstopa pokritost konstante. Pri večjem vzorcu je njena pokritost praktično 0, pri manjših vzorcih pa se opazi vpliv asimtrije ostankov. Bolj ko je porazdelitev ostankov simetrična ($gamma(5,5)$), slabša je pokritost koeficienta $\beta_0$. Predvidevamo, da to niti ni posledica (a)simetričnosti, pač pa pričakovane vrednosti ostankov. Večja kot je pričakovana vrednost ostankov, slabša je pokritost intervalov zaupanja koeficienta $\beta_0$. Pri velikosti vzorca $n=10$ lahko vidimo, da pokritost pada praktično linearno z večanjem parametra $\alpha$ v porazdelitvi ostankov.
Velikost korelacije med pojasnjevalnimi spremenljvkami prav tako vpliva na pokritost koeficienta $beta_0$ - večja kot je korelacija, slabša je pokritost.

### Manj simetrična porazdelitev pojasnjevalnih spremenljivk

Enako kot smo si pogledali za asimetrično porazdelitev pojasnjevalnih spremenljivk si poglejmo še za nekoliko bolj simetrično porazdelitev. 

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```

Ponovno hitro opazimo vpliv velikosti vzorca in korelacije ter da sama asimetrija ostankov nima bistvenega vpliva. Razlike med metodama sta tokrat nekoliko manj opazni, sploh kar se tiče pokritosti, za razlike v širinah intervalov zaupanja pa si moramo pogledati številke v tabelah, kjer bomo lažje razbrali ali so le-te manjše ali večje kot prej. Predvsem lahko opazimo, če primerjamo graf širine intervalov zaupanja s prejšnjih grafom kjer je upoštevana asimetrična porazdelitev, da so intervali v splošnem nekoliko ožji. 

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Številke v tabelah potrjujejo naša opažanja. Pri pokritosti je med metodama `glm` in `ols` manjša razlika in sicer je tokrat pokritost pri metodi `glm` višja za 0 do 1 o.t. Prav tako je manjša razlika v širini intervalov zaupanja. Ponovno pa velja, da se razlika v širini (procentualna) viša z večanjem velikosti vzorca. 

## Vpliv ostranjevanja spremenljivk

Poglejmo si še vpliv izločanja spremenljivk in ali se ta vpliv razlikuje med posameznimi metodami ter pri različnih porazdelitvah pojasnjevalnih spremenljivk. 

### Asimetrična porazdelitev pojasnjevalnih spremenljivk

Za začetek si poglejmo kako se razlikujejo rezultati pri posameznih metodah, potem pa bomo primerjali še posamezne modele z odstranjenimi spremenljivkami pri obeh metodah hkrati.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```

Pri majhnem vzorcu ni velikih razlik pri pokritosti modela, je pa nekaj razlike v širini intervala zapanja, predvsem ob prisotnosti korelacije. Odstranitev spremenljivke $X_3$ nima večjega vpliva, nekoliko višja je pokritost v primeru majhnega vzorca. Močneje vpliva pa ostranitev spremenljivke $X_2$, ki ima dejanski vpliv na odvisno spremenljivko. Najbolj opazen je vpliv v primeru korelacije $0.6$, kjer se precej zniža pokritost. V primeru še višje korelacije se pokritost dvigne, kar je pričakovano, saj so spremenljivke že zelo močno povezane in s tem, ko je ne vključimo, povzročimo manj škode. V splošnem so intervali zaupanja v primeru izključene spremenljivke ožji kot v polnem modelu, vendar pa ne smemo pozabiti, da je v primeru izključitve $X_2$ zato slabša pokritost.

Kar je zanimivo, je, da se pokritost v primeru modela `glm_x2` močno poslabša, ko povečamo vzorec. To je najverjetneje posledica tega, da v primeru večjega vzorca nastane tudi večja razlika med pojasnjevalnimi spremenljivkami.

Najboljši model v tem primeru je, ko izključimo $X_3$, ki nima vpliva, saj imamo podobno pokritost, a ožji interval zaupanja. V naslednji tabeli si lahko bolje ogledamo dejanske številke.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in%  c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Tabele potrjujejo naša opažanja. 

Poglejmo si še razlike med metodama, kjer izločimo eno od spremenljivk. 
Najprej si poglejmo rezultate modela brez $x_2$.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x2 in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in%c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```


```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x2 - pokritost1$ols_x2
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x2 - sirina1$ols_x2
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x2) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Rezultati so podobni kot v primeru polnega modela, večja razlika nastane pri velikih vzorcih. Pri večjih vzorcih glede na polni model nastane večja razlika pri pokritosti. Razlika v širini intervala zaupanja je podobna kot v polnem modelu. 

Kaj pa, če odstranimo $x_3$?

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x3 in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x3 in Gamma(2,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in%c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```


```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x3 - pokritost1$ols_x3
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x3 - sirina1$ols_x3
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x3) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Rezultati so zelo podobni kot v primeru polnega modela.


### Manj asimetrična porazdelitev pojasnjevalnih spremenljivk

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri GLM in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```

Rezultati so na videz podobni tistim z bolj asimetrično porazdelitvijo pojasnjevalnih spremenljivk.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in%  c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Poglejmo si še razlike med metodama pri odstranjenih spremenljivkah. 

Odstranimo $x_2$.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x2 in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in%c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```


```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x2 - pokritost1$ols_x2
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x2 - sirina1$ols_x2
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x2) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Rezultati so podobni kot v primeru polnega modela, vseeno pa je razlika v pokritosti nekoliko višja. Večjo pokritost ima metoda glm, hkrati pa širše intervale zaupanja.

Kaj pa, če odstranimo $x_3$?

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x3 in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez x3 in Gamma(5,5) porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in%c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = "stopnja asimetrije ostankov (alpha)", y = "sirina IZ")
```


```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x3 - pokritost1$ols_x3
kable(pokritost1, booktabs = F, align = "c")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x3 - sirina1$ols_x3
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x3) %>% round(2)
kable(sirina1, booktabs = F, align = "c")
```

Rezultati so zelo podobni kot v primeru polnega modela.


## Analiza variance in velikost učinka

Ker smo že zgoraj ugotovili, da ni večjih razlik pri pokritosti in širini intervalov zaupanja med posameznimi regresijskimi koeficienti, bomo analizo variance naredili na rezultatih za koeficient $\beta_1$. Na spodnjih grafih je tako prikazana velikost učinka pri analizi varianci za pokritost in kasneje še za širino intervala zaupanja glede na vključene spremenljivke. Rezultati iz grafov beremo hierarhično, torej koliko posamezna spremenljivka dodatno pojasni variabilnosti, glede na že upoštevane spremenljivke.

```{r  results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za pokritost intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 1))
simulacija$n <- as.factor(simulacija$n)
simulacija$korelacija <- as.factor(simulacija$korelacija)
simulacija$model <- as.factor(simulacija$model)
simulacija$alpha <- as.factor(simulacija$alpha)
model1 <- aov(pokritost_B1 ~ model * n * korelacija * alpha, data =simulacija)
plot(eta_squared(model1))
```

Iz zgornjega grafa lahko razberemo, da nobena od spremenljivk ne pojasni velikega deleža variabilnosti v pokritosti intervala zaupanja, je pa kar nekaj takih, ki pojasnijo manjši del in ki imajo statistično značilen vpliv (to nam pove povzetek modela anove, ki ni vljučen v prikaz). 

Vidimo, da na začetku model pojasni nekaj manj kot $10\%$ variabilnosti pokritosti intervala zaupanja. Za tem velikost vzorca pojasni okrog $3\%$ preostale variabilnosti in korelacija nekaj manj kot $2.5\%$. Asimetričnost ostankov za tem dodatno ne pojasni nič variabilnosti. Za tem nekoliko večji del variabilnosti (glede na ostale vplive) dodatno pojasnijo še interakcija med modelom in velikostjo vzorca, interakcija med modelom in korelacijo ter na koncu še interakcija modela, velikosti vzorca in korelacije.

Pri tem se moramo zavedati, da znotraj spremenljivke model ni samo \emph{glm} ali \emph{lm}, vendar so upoštevani še ne polni modeli. To poudarimo zato, ker je učinek najverjetneje posledica nepolnih modelov, kjer smo tudi v prejšnjih prikazih zaznali vpliv, medtem ko med \emph{lm} in \emph{glm} modelom nismo opazili razlik.

Vpliv variabilnosti in velikost vzorca smo prav tako zaznali že iz prejšnjih grafov, analiza variance in velikost učinka pa nam naša opažanja še dodatno potrdijo. 
Poglejmo si še velikost učinka pri analizi variance za širino intervalov zaupanja.

```{r results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za širino intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 1))
model2 <- aov(sirina_B1 ~ n * korelacija * model *  alpha, data = as.data.frame(simulacija))
plot(eta_squared(model2))
```

Opazimo, da v tem primeru ni tako veliko spremenljivk, ki bi pojasnjevale variabilnost intervalov zaupanja. Okrog $60\%$ variabilnost opazovane spremenljivke pojasni velikost vzorca, od preostale variabilnosti pa nekaj več kot $20\%$ pojasni korelacija med neodvisnimi spremenljivkami. Model in asimetričnost ostankov dodatno ne pojasnita večjega dela variabilnosti širine intervala zaupanja, okrog $20\%$ pa dodatno pojasnjuje še interakcija med velikostjo vzorca in korelacijo. 
Ponovno nam prikaz in analiza variance potrjujeta naša predhodna opazovanja. 

# Ugotovitve

\textcolor{red}{Vse za prilagodit novim grafom (brez beta0).}

Ugotovili smo, da asimetrija pojasnjevalnih spremenljivk nima pomembnega vpliva na pokritost in širino intervalov zaupanja regresijskih koeficientov.
Prav tako smo ugotovili, da med metodama \emph{glm} in \emph{lm} ni posebnih razlik pri ocenah regresijskih koeficientov. Ker naju je ta ugotovitev presenetila, sva poskusili najti podoben problem v kakšni drugi literaturi in naleteli na delo P. E. Johnsona, ki je prišel do podobnih ugotovitev. Sklenil je, da gama porazdelitev ostankov le redkokdaj vpliva na ocene regresijskih koeficientov, kljub temu, da so kršene nekatere predpostavke.

Večja korelacija med pojasnjevalnimi spremenljivkami poveča širino intervalov zaupanja regresijskih koeficientov z izjemo regresijske konstante. Na slednjo korelacija med pojasnjevalnimi spremenljivkami nima posebnega vpliva. Nekoliko negativno vpliva le na pokritost njenega IZ. Na pokritosti intervalov zaupanja ostalih regresijskih koeficientov pa korelacija med pojasnjevalnimi spremenljivkai nima večjega vpliva.

Asimetrija porazdelitve ostankov vpliva le na pokritost IZ regresijske konstante (v resnici je to najbrž posledica pričakovane vrednosti ostankov in ne asimetrije) in v kombinaciji z večjo korelacijo med pojasnjevalnimi spremenljivkami vpliva na širino IZ regresijskih koeficientov.

Z večanjem velikosti vzorca se pričakovano ožajo intervali zaupanja vseh regresijskih koeficientov. Na pokritost IZ velikost vzorca nima vpliva, razen v primeru regresijske konstante. Pokritost IZ regresijske konstante v primeru gama porazdelitve ostankov nikoli ne doseže velikih vrednosti, saj je kršena predpostavka o ničelni pričakovani vrednosti ostankov.

Izločitev spremenljivke $X_3$ iz modela po pričakovanjih nima bistvenega vpliva na ocene regresijskih koeficientov, saj smo podatke generirali pod predpostavko $\beta_3=0$. Izločitev spremenljivke $X_2$ pri večjih vzorcih vpliva na slabšo pokritost IZ regresijskega koeficienta $\beta_1$.

# Viri

- V. Maver, \emph{Normalni linearni mešani modeli}, diplomsko delo, Fakulteta za matematiko in fiziko, Univerza v Ljubljani, 2018.

- J. Jiang, \emph{Linear and Generalized Linear Mixed Models and Their Applications}, Springer Series in Statistics, Springer Science + Business Media, LLC, New York, 2007.

- \emph{glm}, v: RDocumentation, [ogled 30.12.2020], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/glm}.

- \emph{lm}, v: RDocumentation, [ogled 30.12.2020], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm}.

- \emph{confint}, v: RDocumentation, [ogled 02.01.2020], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/confint}

- M. Raič, \emph{O linearni regresijji}, 2014. Najdeno na spletnem naslovu:
\url{http://valjhun.fmf.uni-lj.si/~raicm/Odlomki/Linearna_regresija.pdf}

- L. Pfajfar, \emph{Osnovna ekonometrija}, učbeniki Ekonomske fakultete, Ljubljana, 2018.

- P. E. Johnson, \emph{GLM with a Gamma-distributed Dependent Variable}, [ogled 05.01.2020], dostopno na \url{https://pj.freefaculty.org/guides/stat/Regression-GLM/Gamma/GammaGLM-01.pdf?fbclid=IwAR14W34VhGzyG0wPiqNTk1hWjIToAug6a2TsPsTeZKLj_ntfTxaR1Aowiko}

# Priloge

Vsa uporabljena koda se nahaja v priloženi datoteki `Simulacije.R`.

Sledi izpis ANOVE za prvi in drugi model.

```{r}
summary(model1)
```

```{r}
summary(model2)
```

