---
title: "Vpliv kršenja predpostavk linearne regresije na njene rezultate"
subtitle: "Seminarska naloga pri predmetu Računsko zahtevne metode"
author: "Anja Žavbi Kunaver in Vesna Zupanc"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: header.tex
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
# Potrebni paketi:
library(ggplot2) # za risanje grafov
library(knitr) # lepši prikaz tabel
library(gridExtra) # lepši prikaz tabel
library(grid)
library(foreign)
library(MASS)
library(lattice)
library(multiUS) # npr. za izpis p-vrednosti
library(rms)
library(tidyverse)
library(reshape2)
library(psych)
library(Hmisc)
library(rgl)
library(blockmodeling)
library(mclust)
library(sn)
library(corrplot)
```

\pagebreak

# Uvod

Obravnavana metoda je linearna regresija in zanima nas, kako kršenje predpostavk (konkretneje nenormalna porazdelitev ostankov in močna korelacija med pojasnjevalnimi spremenljivkami) vpliva na njene rezultate.
Preverjali bomo vpliv različnih dejavnikov na pristranost in širino intervalov zaupanja regresijskih koeficientov.
Ti dejavniki so velikost vzorca, moč korelacije med pojasnjevalnimi spremenljivkami, asimetrija porazdelitve ostankov, asimetrija porazdelitve pojasnjevalnih spremenljivk ter število vključenih spremenljivk v modelu.
Poleg tega bomo preverjali, če probleme kršenja predpostavk lahko (vsaj delno) odpravimo z uporabo posplošenih linearnih modelov. Ob koncu naloge bomo preverili še, kako dobro se \emph{link} funkcija v posplošenih modelih obnese v primeru logaritemske transoformacije pojasnjevalne spremenljivke.

# Teoretični del

Naloga je osredotočena na linearno regresijo, ki spada pod posplošene linearne modele. V tem poglavju so najprej bolj splošno predstavljeni posplošeni linearni modeli, nato pa podrobneje model linearne regresije in metode, s pomočjo katerih lahko ocenjujemo regresijske koeficiente.

## Posplošeni linearni modeli

Posplošeni linearni mešani model izrazimo kot $$Y=X\beta+Z\alpha+\epsilon,$$ kjer je $Y$ opazovani slučajni vektor, $X$ matrika znanih vrednosti pojasnjevalnih spremenljivk, $\beta$ neznan vektor regresijskih koeficientov (fiksni učinki), $Z$ znana matrika, $\alpha$ vektor naključnih učinkov in $\epsilon$ vektor napak. $\alpha$ in $\epsilon$ sta neopazovana. Predpostavimo, da sta nekorelirana.

V matrični obliki model izgleda takole: $$
\begin{bmatrix}
Y_{1} \\
Y_{2} \\
\vdots \\
Y_{n}
\end{bmatrix}
=
\begin{bmatrix}
X_{1,1} & X_{1,2} & \dots & X_{1,p} \\
X_{2,1} & X_{2,2} & \dots & X_{2,p} \\
\vdots & \vdots &  & \vdots \\
X_{n,1} & 0 & \dots & X_{n,p}
\end{bmatrix}
\begin{bmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{p}
\end{bmatrix}
+
\begin{bmatrix}
Z_{1,1} & Z_{1,2} & \dots & Z_{1,q} \\
Z_{2,1} & Z_{2,2} & \dots & Z_{2,q} \\
\vdots & \vdots &  & \vdots \\
Z_{n,1} & Z_{n,2} & \dots & Z_{n,q}
\end{bmatrix}
\begin{bmatrix}
\alpha_{1} \\
\alpha_{2} \\
\vdots \\
\alpha_{q}
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2} \\
\vdots \\
\epsilon_{n}
\end{bmatrix}
.$$

Linearni mešani modeli se delijo na Gaussove ali normalne in ne-Gaussove.
Pomembna predpostavka pri normalnih linearnih mešanih modelih je normalna porazdeljenost vektorja slučajnih učinkov $\alpha \sim N(0,\sigma^2 I_q)$ in vektorja slučajnih odstopanj $\epsilon \sim N(0,\tau^2 I_n)$, ki nista nujno enakih razsežnosti. Druga pomembna predpostavka je neodvisnost slučajnih vektorjev $\alpha$ in $\epsilon$. Prednost uporabe nenormalnih linearnih mešanih modelov pred normalnimi je v tem, da so bolj fleksibilni za modeliranje
(Maver, 2018, str. 6).

## Linearna regresija

Linearna regresija je statistični model, ki ga v najbolj enostavni obliki lahko zapišemo kot:
$$Y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i},$$ kjer so $\epsilon_{i}$ med seboj neodvisne slučajne spremenljivke, $x_{i}$ pa dane vrednosti.
Velja $\epsilon_{i} \sim N(0, \sigma^{2})$ za vsak $i$ in tako $Y_{i} \sim N(\beta_{0} + \beta_{1}x_{i},\ \sigma^{2})$. Model lahko razširimo na več linearnih parametrov: $$Y_{i} = \beta_{0} + \beta_{1} x_{i1} + \dots +\beta_{p} x_{ip} + \epsilon_{i},$$ kjer so $\epsilon_{i}$ neodvisne enako porazdeljene slučajne spremenljivke, za $1\leq i\leq n$.

Lahko ga zapišemo tudi v matrični obliki: $$Y = X \beta + \epsilon.$$

Med temeljne predpostavke regresijskega modela spada predpostavka, da med neodvisnimi spremenljivkami ni popolne kolinearnosti ali multikolinearnosti. Najbolj tipičen vzrok za kršenje te predpostavke je, da smo v model kot neodvisni vključili dve spremenljivki, med katerima obstaja močna linearna povezanost. Do multikolinearnosti pride tudi, če v model vključimo več spremenljivk kot je velikost vzorca. Na multikolinearnost posumimo, če se v modelu determinacijski koeficient izkaže za statistično značilnega, od regresijskih koeficientov pa nobeden.

Opazovanja so med seboj neodvisna. V primeru kršenja te predpostavke je smiselno uporabiti posplošene linearne modele, običajno longitudinalni (vzdolžni) model.
Vse predpostavke linearnega regresijskega modela so navedene v naslednjem razdelku.

## Metoda najmanjših kvadratov (MNK)
Pri 16 letih jo je odkril nemški matematik Carl F. Gauss. Zaradi svojih lastnosti je najbolj razširjena metoda ocenjevanja regresijskih koeficientov
(Pfajfar, 2018, str.53).

Pri MNK na primeru osnovnega regresijskega modela velikosti $p=1$ iščemo $\beta_{0}$ in $\beta{1}$ tako, da bo vsota kvadratov ostankov najmanjša možna. Pri danih $(x_{i}, y_{i})$ torej iščemo $$\min_{\beta_{0}, \beta_{1}} \sum_{i=1}^{n} (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}.$$

Za razumevanje oznak v predpostavkah metode ločimo dva modela, in sicer linearni vzorčni regresijski model $y_i=b_1+b_2x_i+e_i$ in linearni populacijski regresijski model $y=\beta_1+\beta_2x_i+u_i$.
Pfajfar (2018) navaja naslednje predpostavke metode najmanjših kvadratov:

- linearnost regresijskega modela: $y=\beta_1+\beta_2x_i+u_i$

- ničelna povprečna vrednost $u_i$: $E(u_i)=0$

- homoskedastičnost: $Var(u_i)=E(u_i^2)=\sigma^2$

- odsotnost avtokorelacije: $cov(e_i, e_j|x_i, x_j)=0$ za vsak $i\ne j$

- nekoreliranost med pojasnjevalnimi spremenljivkami in slučajno spremenljivko $u$: $Cov(x_2,u)=Cov(x_3,u)=...=Cov(x_k,u)=0$

- število opazovanj mora presegati število ocenjenih parametrov oz. pojasnjevalnih spremenljivk: $n>k$

- $Var(X)$ je končno pozitivno število

- pravilno specificiran regresijski model: vključene vse relevantne pojasnjevalne spremenljivke in izbrana ustrezna funkcijska oblika modela

- odsotnost popolne multikolinearnosti: $\lambda_1X_1+\lambda_2X_2+...+\lambda_kX_k=0$

- slučajna spremenljivka $u$ je normalno porazdeljena: $u_i \sim N(0,\sigma_u^2)$.
Posledično je pogojna porazdelitev odvisne spremenljivke $y$ tudi normalna in sicer $N(\beta_1x_{1i}+...+\beta_kx_{ki},\sigma_u^2)$

## Metoda iterativnega uteženega povprečja najmanjših kvadratov (IWLS)

Naj bo $Y$ vektor meritev in $X$ matrika znanih konstant. Naj bo $E(Y) = X\beta$, kjer $\beta$ kot do sedaj predstavlja vektor neznanih regresijskih koeficientov. Cenilko za $\beta$ se po uteženi metodi najmanjših kvadratov dobi z minimizacijo izraza 
\begin{equation}
(Y - X\beta)'W(Y - X\beta),
\end{equation}
kjer je $W$ znana simetrična matrika uteži.

Brez škode za splošnost naj bo rang matrike $X$ poln in naj velja rang$X=p$.
Potem je za vsako nesingularno (simetrično) matriko $W$ minimum izraza (1) enak 
\begin{equation}
\hat{\beta}_W = (X'WX)^{-1}X'WY.
\end{equation}
Cenilko za $\beta$ po običajni metodi najmanjših kvadratov (ang. ordinary least squares, OLS) se dobi kot poseben primer, za $W=I$:
\begin{equation}
\hat{\beta}_I=(X'X)^{-1}X'Y .
\end{equation}

Izkaže se, da je v smislu čim manjše variance optimalna izbira za matriko $W$ matrika  $W = V^{-1},$ kjer je $V = Var(Y)$. Tako dobljena cenilka za parameter $\beta$ je najboljša, saj je z njeno uporabo dosežena najmanjša možna variabilnost med vsemi drugimi alternativami. V tem primeru se dobljeni cenilki za $\beta$ reče najboljša linearna nepristranska cenilka ali $BLUE$ (ang. best linear unbiased estimator):
\begin{equation}
\hat{\beta}_{BLUE} = (X'V^{-1}X)^{-1}X'V^{-1}Y.
\end{equation}

V enačbi za $\beta_{BLUE}$ nastopa tudi $V$, ki pa tipično ni znana. Zaradi poenostavitve je v nadaljevanju prikazan postopek izračuna cenilke $BLUE$ zgolj na uravnoteženem primeru.
Naj bo $Y_{ij}, j = 1,..., \tilde{m}$, vektor meritev na $i-$tem posamezniku, kjer je $\tilde{m}$ fiksno število. V uravnoteženem primeru so na vseh posameznikih meritve pridobljene ob določenih časovnih trenutkih $t_1, . . . , t_{\tilde{m}}$ . Za $i-$tega posameznika se lahko vektor
meritev zapiše kot $Y_i = (Y_{ij})_{\leq j\leq \tilde{m}}, i = 1,...,n$. Naj bodo $Y_1, . . . , Y_n$ med seboj neodvisni in naj za njih velja $E(Y_i) = X_i\beta$ in $Var(Y_i) = V_0$. Tu je $X_i$ matrika znanih konstant in $V_0 = (v_{qr})_{1\leq q, r\leq \tilde{m}}$ neznana variančno kovariančna matrika. Iz tega sledi, da je $V = diag(V_0, . . . , V_0)$. Ker je število meritev $\tilde{m}$ na vsakem posamezniku fiksno, je mogoče poiskati dosledno cenilko za $V$. Če bi bil parameter $\beta$ znan, bi bila dosledna cenilka za $V$ kar $$\hat{V} = diag(\hat{V}_0,...,\hat{V}_0),$$
kjer je
\begin{equation}
\hat{V}_0 =\frac{1}{n}\sum^n_{i=1} (Y_i - X_i\beta)(Y_i - X_i\beta)'.
\end{equation}

Če bi bila $V$ znana, bi lahko za izračun najboljše linearne nepristranske cenilke za $\beta$ uporabili (4), če pa bi poznali $\beta$, bi z (5) dobili dosledno cenilko za $V$.

Metodi, kjer ni treba poznati ne $\beta$, ne $V$, pa se reče iterativno uteženo povprečje najmanjših kvadratov (ang. iterative weighted least squares, IWLS). Postopek omenjene metode je sledeč:

* Najprej se izračuna cenilka za $\beta$ po običajni metodi najmanjših kvadratov s pomočjo (3).
* Nato se izračuna $\hat{V}$ po (5), kjer je $\beta$ zamenjan z $\hat{\beta}_I$ izračunanim en korak prej.
* V zadnjem koraku pa se na desni strani (4) matriko $V$ zamenja z njeno cenilko $\hat{V}$, izračunano na prejšnjem koraku.

Na tak način se dobi cenilka za $\beta$ po prvi iteraciji, nato pa se postopek ponavlja.
Pod predpostavko normalnosti se izkaže, da če IWLS konvergira, bo cenilka v limiti enaka cenilki, dobljeni po metodi največjega verjetja (celotno podpoglavje je povzeto po Maver, 2018, strani 19-21).


# Generiranje podatkov

## Parametri

Fiksni parametri pri generiranju podatkov so sledeči:

- formula za generiranje podatkov:

$$y_i = 1 + x_{1i} + x_{2i} + 0x_{3i} + \epsilon_i .$$

Pri generiranju podatkov bomo spreminjali sledeče:

- velikost vzorca $n \in \{10, 50, 100, 500, 1000\}$;

- korelacija med pojasnjevalnimi spremenljivkami ($cor \in \{0, 0.3, 0.6, 0.9\}$);

- porazdelitev pojasnjevalnih spremenljivk:
  $X_j \sim Gamma(\delta,5)$, $j=1,2,3$, $\delta=2,5$;

- porazdelitev napak ($Gamma(\alpha, 5)$), kjer bomo parameter $\alpha$ spreminjali tako, da dobimo različno močno asimetrične porazdelitve ($\alpha \in \{1, 3, 5\}$;

- v modelu ne upoštevamo vseh neodvisnih spremenljivk (spreminjamo število spremenljivk, ki jih upoštevamo): enkrat vključimo vse spremenljivke, enkrat izločimo $X_3$ (ki nima vpliva na odzivno spremenljivko), enkrat pa izločimo $X_2$.

Pri generiranju koreliranih gama spremenljivk lahko uporabimo sledečo lastnost: Če $X_i \sim Gamma(k_i, \theta)$, potem je
$$\sum_{i=1}^{n} X_i \sim Gamma(\sum_{i=1}^{n} k_i, \theta).$$

Pri pregledu literature sva ugotovili, da za generiranje odvisnih gama spremenljivk lahko uporabimo kar funkcijo \emph{rmvgamma()} iz paketa `lcmix` in si s tem olajšamo delo pri generiranju podatkov. Funkcija sprejme naslednje parametre: $n$ (število vektorjev, ki jih želimo generirati), $corr$ (korelacijska matrika) ter parametra `shape` in `rate`, ki sta privzeto nastavljena na 1. Kot rezultat dobimo matriko z $n$ vrsticami in $ncol(corr)$ stolpci, v kateri so vrednosti koreliranih gamma spremenljivk (mvgama, 2020).
Koda, na podlagi katere funkcija generira vrednosti, je predstavljena v viru lcmix, 2021.

Prikaz porazdelitev, na podlagi katerih so generirane pojasnjevalne spremenljivke in ostanki, je predstavljen na naslednji sliki.

```{r fig.cap="Različne porazdelitve gama"}
par(mfrow=c(2,2))

x <- seq(0,5,.001)
y <- dgamma(x,1,5)
plot(x,y, main="gamma(1,5)")

x <- seq(0,5,.001)
y <- dgamma(x,2,5)
plot(x,y, main="gamma(2,5)")

x <- seq(0,5,.001)
y <- dgamma(x,3,5)
plot(x,y, main="gamma(3,5)")

y <- dgamma(x,5,5)
plot(x,y, main="gamma(5,5)")
```


V poglavju `Transformacija odzivne spremenljivke` obravnavamo poseben primer, kjer za generiranje podatkov uporabimo sledečo formulo:
$$y_i=exp(1+ x_{1i} + x_{2i} + 0x_{3i} + \epsilon_i )$$

Enako kot prej imamo 5 različnih velikosti vzorcev, 4 različne korelacije med pojasnjevalnimi spremenljivkami, 3 porazdelitve ostankov, 2 porazdelitvi pojasnjevalnih spremenljivk in 2 metodi, ne obravnavamo pa modelov z manjkajočimi spremenljivkami. Model \emph{lm} oblikujemo po sledeči formuli: $$log(Y)\sim \beta X + \epsilon,$$ v modelu \emph{glm} pa uporabimo kar $$Y\sim \beta X + \epsilon$$ in za upoštevanje logaritemske transformacije uporabimo `family=gamma(link="log")` (podrobneje razloženo v naslednjem razdelku). Zanima nas predvsem razlika med tema dvema pristopoma.

## Funkciji lm() in glm()

Funkcija \emph{lm()} se uporablja za ocenjevanje linearnih modelov. Avtomatično uporablja osnovno metodo najmanjših kvadratov, lahko pa nastavimo tudi na metodo uteženih najmanjših kvadratov
(lm iz RDocumentation, 2020). V tej seminarski nalogi uporabljamo samo osnovno metodo najmanjših kvadratov.

V linearnem modelu $Y_i = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip} + \epsilon_{i}$ velja
$E(Y_i) = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip}$. Slednjo enačbo lahko z uporabo primerno definirane funkcije $g$ posplošimo do $$g(E(Y_i)) = \beta_0 + \beta_1  X_{i1} + ... + \beta_p X_{ip}.$$ Tu z indeksom $i$ označujemo i-tega posameznika. Prejšnja enačba je poseben primer, kjer je $g$ identiteta.
Z uporabo funkcije \emph{glm()} in znotraj primerno definirane funkcije $g$ lahko generiramo več posplošenih linearnih modelov. Prednost te funkcije je tudi v tem, da lahko poleg normalne porazdelitve nastavimo še katero drugo porazdelitev ostankov. To naredimo tako, da npr. v primeru gama porazdelitve ostankov znotraj funkcije \emph{glm()} definiramo \emph{family=Gamma(link="identity")}. Tu \emph{identity} pomeni, da za funkcijo \emph{g} vzamemo kar identiteto, možne izbire pa so še \emph{log}, \emph{inverse}, \emph{logit} in druge (možna je tudi nova definicija). Parameter \emph{family} določa porazdelitev ostankov. Na voljo imamo normalno, binomsko, gama, inverzno normalno, Poissonovo, kvazi (poleg \emph{link} funkcije nastavimo še varianco), kvazi-binomsko in kvazi-Poissonovo. Funkcija \emph{glm} parametre modela ocenjuje po metodi iterativnega uteženega povprečja najmanjših kvadratov
(glm iz RDocumentation, 2020).

## Ocenjevanje intervalov zaupanja

Pri izračunih intervalov zaupanja uporabimo funkciji \emph{confint} in \emph{confint.default}. Prva predpostavlja normalnost, druga pa temelji na asimptotski normalnosti in računa intervale zaupanja na podlagi standardnih napak. Funkcija \emph{confint} je posebej prilagojena funkcijama \emph{lm} in \emph{glm} - če uporabimo model, ocenjen na enega od teh dveh načinov, funkcija avtomatično pokliče \emph{confint.lm} oziroma \emph{confint.glm} (confint iz RDocumentation, 2020). Iz neznanih razlogov se pri posplošenih linearnih modelih občasno pojavljajo težave, zato v primeru "error" uporabimo \emph{confint.default}. Če pri generiranju podatkov ne skaliramo variance, ni težav z uporabo funkcije \emph{confint}, vendar pa je v tem primeru potrebno paziti pri interpretaciji rezultatov. V kodi za simulacije je nastavljena zanka, ki v primeru napake uporabi funkcijo \emph{confint.default} in neuspeh funkcije \emph{confint} zabeleži pod številko 0 v stolpcu "confint_success" (če ni težav, zabeleži vrednost 1).

## Skaliranje variance

Ker zaradi spreminjanja parametrov v gama porazdelitvi ne spreminjamo samo asimetričnosti, ampak tudi varianco in ker vemo, da ima velikost variance vpliv na model, smo spremenljivke napak skalirali. Napake smo skalirali tako, da smo vrednosti delili s teoretično standardno napako in tako poskrbeli, da imajo vse porazdelitve napak enako varianco. 

Podrobnejši pregled gama porazdelitve z vsemi dokazi je pripravil K. Siegrist (2020). Če je spremenljivka $X$ porazdeljena po $Gamma(\alpha, \beta)$, je njena pričakovana vrednost enaka $\frac{\alpha}{\beta}$, varianca pa $\frac{\alpha}{\beta^2}$. Za to spremenljivko in neko konstanto $c$ velja $cX \sim Gamma(\alpha, \frac{\beta}{c})$. To lastnost uporabimo pri skaliranju variance in sicer tako, da vrednosti delimo s $\sqrt{\frac{\alpha}{\beta^2}}=\frac{\sqrt{\alpha}}{\beta}$. Tako dobimo skalirano spremenljivko $\frac{\beta}{\sqrt{\alpha}}X \sim Gamma(\alpha, \sqrt{\alpha})$.

```{r fig.cap="Skalirane porazdelitve ostankov"}
par(mfrow=c(2,2))

y <- dgamma(x,1,sqrt(2))
plot(x,y, main = expression(paste("gamma(1,",sqrt(1),")",sep='')))

y <- dgamma(x,3,sqrt(2))
plot(x,y, main = expression(paste("gamma(3,",sqrt(3),")",sep='')))

y <- dgamma(x,5,sqrt(5))
plot(x,y, main = expression(paste("gamma(5,",sqrt(5),")",sep='')))
```

V poglavju `Transformacija odzivne spremenljivke` skaliranja variance nismo uporabili. Razlog je v tem, da program občasno vrača opozorila `task failed - "NA/NaN/Inf in 'x'"`. Ker nas v resnici zanima le razlika med pristopoma \emph{lm} in \emph{glm}, nas različna varianca ostankov pri interpretaciji ne zmoti.

## Pričakovanja

Pri večji korelaciji med pojasnjevalnimi spremenljivkami pričakujemo širše intervale zaupanja regresijskih koeficientov ne glede na izbiro metode. Zaradi večje širine intervalov zaupanja ne pričakujemo večjih sprememb v pokritosti.

Večje razlike med metodami pričakujemo predvsem pri manjših velikostih vzorcev in večji asimetriji porazdelitve ostankov. Pri dovolj velikih vzorcih pričakujemo podobne rezultate obeh metod, prav tako pa seveda tudi manjšo variabilnost rezultatov.

Pričakujemo, da lahko kršenje predpostavke o normalni porazdeljenosti ostankov rešimo z uporabo posplošenih linearnih modelov z ustrezno definirano porazdelitvijo ostankov oz. odzivne spremenljivke. Pričakujemo, da bolj kot bo porazdelitev ostankov asimetrična (manjša vrednost parametra $\alpha$), slabši bodo rezultati funkcije \emph{lm()} in posledično večje razlike med rezultati funkcij \emph{lm()} in \emph{glm()}.

V primeru, ko iz modela izločimo spremenljivko $X_3$ in nimamo velikih korelacij med pojasnjevalnimi spremenljivkami, ne pričakujemo posebnih sprememb v rezultatih, saj spremenljivka nima vpliva na vrednost pojasnjevalne spremenljivke. Pravzaprav lahko model brez te spremenljivke označimo kot "pravi" model. V primeru visoke korelacije med pojasnjevalnimi spremenljivkami v polnem modelu pričakujemo večje napake pri ocenjevanju regresijskih koeficientov (posledično slabšo pokritost ali širše IZ), zato predvidevamo, da se bo model brez vključene spremenljivke $X_3$ bolje obnesel.
V primeru, ko izločimo spremenljivko $X_2$, pa pričakujemo večje spremembe v rezultatih - širše intervale zaupanja regresijskih koeficientov in slabšo pokritost.

Porazdelitev pojasnjevalnih spremenljivk preverimo za dve porazdelitvi - $Gamma(2,5)$, ki je precej asimetrična in $Gamma(5,5)$, ki je zelo podobna normalni porazdelitvi. Zanima nas, če in kako asimetrija pojasnjevalnih spremenljivk vpliva na ocene regresijskih koeficientov. Pričakujemo, da bo v primeru asimetrične porazdelitve prišlo do manjše pokritosti in večje širine intervalov zaupanja.

V primeru transformacij pričakujemo podobne rezultate med metodama \emph{lm} in \emph{glm}, morda boljše pri slednji. Prednost \emph{glm} je v tem, da lahko nastavimo gama porazdelitev ostankov, za \emph{link} funkcijo pa pričakujemo, da se bo obnesla približno tako dobro kot logaritemska transformacija formule v \emph{lm} funkciji.

# Predstavitev rezultatov

```{r}
# potrebni paketi
library("MASS")
library("sjstats")
library("doRNG")
library("parallel")
library("doParallel")
library("reshape2")
library("ggplot2")
library("see")
library("effectsize")
library("lcmix")
library("dplyr")

# Uvozimo rezultate simulacij:
load("rezultati_simulacije.R")
for (j in c(2:5,9:15)){ # stolpce pretvorimo v numeric, ker so shranjeni kot character
  simulacija[,j] <- as.numeric(simulacija[,j])
}
for (j in c(6:8)){ # stolpce pretvorimo v factor, ker so shranjeni kot character
  simulacija[,j] <- as.logical(simulacija[,j])
  simulacija[,j] <- as.numeric(simulacija[,j])
}
simulacija$model <- as.factor(simulacija$model) # ime modela pretvorimo v tip factor
```

Izvedli smo 1000 ponovitev simulacij. Na vsakem koraku obravnavamo 4 različne korelacije, 5 različnih velikosti vzorca, 3 različne porazdelitve ostankov, 2 različni porazdelitvi pojasnjevalnih spremenljivk, 2 metodi (\emph{lm} in \emph{glm}) in 3 modele glede na število spremenljivk - iz tega sledi, da dobimo tabelo s 720 000 vrsticami.

Intervali zaupanja so generirani s pomočjo funkcije \emph{confint()} oziroma v primeru, ko naletimo na `error`, s funkcijo \emph{confint.default()}. Primerov, ko je bilo potrebno uporabiti slednjo funkcijo, je `r nrow(simulacija)-sum(simulacija$confint_succes=="1")`, kar znaša `r round((nrow(simulacija)-sum(simulacija$confint_succes=="1"))/nrow(simulacija)*100,3)` % vseh ponovitev simulacij.

Rezultati se nanašajo na veliko parametrov in več različnih načinov modeliranja, zato si jih bomo ogledali na različne načine.
Ker imata koeficienta $X_1$ in $X_2$ enak vpliv na odzivno spremenljivko, $X_3$ pa nima vpliva, se lahko osredotočimo le na koeficient $\beta_1$.

## Polni model

### Zelo asimetrična porazdelitev pojasnjevalnih spremenljivk

Za začetek si poglejmo rezultate oz. izbrane mere na polnem modelu, kjer so upoštevane vse spremenljivke pri bolj asimetrični porazdelitvi pojasnjevalnih spremenljivk $Gamma(2,5)$. Naslednje dva grafa tako prikazujeta pokritost in širine intervalov zaupanja za koeficient $\beta_1$ za posamezen model (\emph{gls} ali \emph{ols}) glede na velikost vzorca in korelacijo. Črna črtkana črta na grafu pokritja označuje željeno pokritje (0.95).

```{r}
pokritost <- c("pokritost_B1", "pokritost_B2", "pokritost_B3")
sirina <- c()
data_pokritost <- melt(simulacija, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = pokritost)
data_pokritost$korelacija = data_pokritost$korelacija %>% round(1)
sirina <- c("sirina_B1", "sirina_B2", "sirina_B3")
data_sirina <- melt(simulacija, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = sirina)
data_sirina$korelacija = data_sirina$korelacija %>% round(1)
```

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c("ols","glm"), variable == "pokritost_B1", alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed") + 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```


```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Iz slik 3 in 4 lahko vidimo, da imata na ocene regresijskih koeficientov precej očiten vpliv pri obeh metodah (\emph{glm} in \emph{ols}) korelacija in velikost vzorca. Z večanjem korelacije se sama pokritost bistveno ne spremeni, se pa močno povečajo širine intervalov zaupanja. Z večanjem velikosti vzorca pridobimo večjo pokritost pri \emph{glm} metodi in ožje intervale zaupanja pri obeh metodah. Pri različnih stopnjah asimetrije ostankov ni opaziti bistvenega vpliva na rezultate.
Razberemo lahko, da ima model \emph{glm} pri dovolj velikih vzorcih nekoliko boljšo pokritost. Velja pa tudi, da pri \emph{glm} modelu dobimo nekoliko širše intervale zaupanja, od koder najverjetneje izhaja tudi boljša pokritost. Intervali zaupanja pri metodi \emph{ols} so ožji, posledično pa interval zaupanja za koeficient večkrat ne vsebuje prave vrednosti koeficienta. Razlike v pokritosti sicer niso velike, vseeno pa so prisotne.
Zanimivo je dejstvo, da ima samo pri majhnem vzorcu ($n=10$) metoda \emph{ols} širši interval zaupanja in boljšo pokritost kot \emph{glm}.

Za manjše število izbranih parametrov $n$ in korelacije so rezultati prikazani tudi v spodnjih dveh tabelah, kjer lažje razberemo razlike. Prikazane razlike so v obeh primerih razlike \emph{glm} glede na \emph{ols} metodo, v tabeli s širino intervalov zaupanja so prikazane še razlike v deležu glede na širino intervala zaupanja pri metodi \emph{ols}.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Velja torej, da ima v povprečju \emph{glm} metoda od 1 do 2 ostotni točki boljšo pokritost (z izjemo najmanjšega opazovanega vzorca), vendar pa ima tudi do $14\%$ širše intervale zaupanja. Ta razlika med intervali zaupanja se najbolj poveča z večanjem velikosti vzorca. Visoka korelacija v kombinaciji z majhnim vzorcem je najbolj problematična, saj imamo tam tako manjšo pokritost kot tudi precej široke intervale zaupanja. 


### Manj asimetrična porazdelitev pojasnjevalnih spremenljivk

Enako kot smo si pogledali za $Gamma(2,5)$ porazdelitev pojasnjevalnih spremenljivk, poglejmo še za nekoliko bolj simetrično porazdelitev $Gamma(5,5)$. 

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Ponovno hitro opazimo vpliv velikosti vzorca in korelacije ter dejstvo, da sama asimetrija ostankov nima bistvenega vpliva na ocene regresijskih koeficientov. Razlike med metodama so tokrat nekoliko manj očitne, vseeno pa opazimo, da je pri \emph{glm} metodi pokritost pri večjih vzorcih boljša od pokritosti pri \emph{ols} metodi.
Kar se tiče širine IZ, so tokrat razlike res majhne z izjemo majhnega vzorca ($n=10$) in visoke korelacije ($0.9$). Ponovno ima torej metoda \emph{glm} pri večjih vzorcih boljšo pokritost (a širše intervale zaupanja) kot druga metoda.
Za razlike v širinah intervalov zaupanja si moramo pogledati številke v tabelah, kjer bomo lažje razbrali ali so le-te manjše ali večje kot prej. Predvsem lahko opazimo, da če primerjamo graf širine intervalov zaupanja s prejšnjih grafom, kjer je upoštevana asimetrična porazdelitev, so intervali v splošnem nekoliko ožji.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri polnem modelu in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Številke v tabelah potrjujejo naša opažanja. Med metodama `glm` in `ols` je pri pokritosti manjša razlika in sicer je pri dovolj velikih vzorcih tokrat pokritost pri metodi `glm` višja za do 1 o.t. Prav tako je manjša razlika v širini intervalov zaupanja. Ponovno pa velja, da se (procentualna) razlika v širini z večanjem velikosti vzorca povečuje v prid metode \emph{glm}.

## Vpliv odstranjevanja spremenljivk

Poglejmo si še vpliv izločanja spremenljivk. Zanima nas, kakšen je ta vpliv, ali se razlikuje med metodama in ali se razlikuje med različno asimetričnima porazdelitvama pojasnjevalnih spremenljivk. 

### Zelo asimetrična porazdelitev pojasnjevalnih spremenljivk

Za začetek si poglejmo, kako se razlikujejo rezultati pri posamezni metodi, potem pa bomo primerjali še posamezne modele z odstranjenimi spremenljivkami pri obeh metodah hkrati. Zaradi podobnosti med rezultati obeh metod so rezultati modelov (glede na število spremenljivk) predstavljeni le za \emph{glm} metodo.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_sirina %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Pri majhnem vzorcu ni velikih razlik pri pokritosti glede na model, je pa predvsem ob prisotnosti višje korelacije nekaj razlik v širini intervalov zapanja. Odstranitev spremenljivke $X_3$ nima večjega vpliva na pokritost, dobimo pa nekoliko ožje intervale zaupanja. Večji vpliv ima odstranitev spremenljivke $X_2$, ki ima dejanski vpliv na odvisno spremenljivko. V splošnem so intervali zaupanja v primeru izključene spremenljivke pri višji korelaciji ožji kot v polnem modelu, vendar pa ne smemo pozabiti, da je v primeru izključitve $X_2$ zato slabša pokritost.
Kar je zanimivo, je, da se pokritost v primeru modela `glm_x2` močno poslabša, ko povečamo vzorec (vendar le v primeru neničelne korelacije).

Najboljši model dobimo, ko izključimo $X_3$, ki nima vpliva, saj imamo podobno pokritost kot pri polnem modelu, a ožji interval zaupanja. V naslednjih tabelah, ki potrjujo naša opažanja, si lahko bolje ogledamo dejanske številke.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in%  c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Poglejmo si še razlike med rezultati funkcij \emph{glm} in \emph{lm}, ko v modelu izključimo eno od spremenljivk. 
Najprej si poglejmo rezultate modela brez $X_2$.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in%c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Podobno kot pri polnih modelih ima metoda \emph{glm} pri dovolj velikih vzorcih boljšo pokritost, a širše intervale zaupanja kot \emph{ols}. Razlika med metodama je bolj očitna pri višji korelaciji.
Pri večjih vzorcih glede na polni model nastane večja razlika v pokritosti. Razlika v širini intervala zaupanja je podobna kot v polnem modelu. Sledita še tabeli, v katerih so bolje razvidne številske razlike med metodama.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x2 - pokritost1$ols_x2
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x2 - sirina1$ols_x2
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x2) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Kaj pa, če odstranimo $X_3$?

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_sirina %>% filter(model %in%c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Rezultati so zelo podobni kot v primeru polnega modela. Model \emph{glm} ima z izjemo majhnega vzorca ($n=10$) pokritost celo boljšo od željene, intervali zaupanja pa so v primerjavi z \emph{ols} nekoliko širši.
V primeru zelo majhnega vzorca je očitno bolje izbrati \emph{ols} model, saj ima kljub nekoliko širšim IZ za 3 o.t. boljšo pokritost.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x3 - pokritost1$ols_x3
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x3 - sirina1$ols_x3
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x3) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```


### Manj asimetrična porazdelitev pojasnjevalnih spremenljivk

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}

ggplot(data_sirina %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Rezultati so na videz podobni tistim z bolj asimetrično porazdelitvijo pojasnjevalnih spremenljivk. Vplivi ostalih parametrov, ki jih spreminjamo, ostajajo enaki, smo pa z večjo asimetrijo porazdelitve pojasnjevalnih spremenljivk dobili ožje intervale zaupanja in malenkost slabšo pokritost (razlika je približno $1$ o.t. in je vidna, če primerjamo številske tabele).

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('glm','glm_x2','glm_x3'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in%  c('glm','glm_x2','glm_x3'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri GLM in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Poglejmo si še razlike med metodama pri odstranjenih spremenljivkah.
Najprej iz modela odstranimo $X_2$.

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_sirina %>% filter(model %in%c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Pri pokritosti v modelih brez $X_2$ opazimo večje razlike v primerjavi z bolj asimetrično porazdelitvijo pojasnjevalnih spremenljivk. Tokrat imamo predvsem pri večjih vzorcih pokritost precej slabšo kot prej (primerjava tabel 7 in 13), vendar pa to lahko pripišemo predvsem ožjim intervalom zaupanja (primerjava tabel 8 in 14).

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x2 - pokritost1$ols_x2
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x2','glm_x2'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x2 - sirina1$ols_x2
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x2) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_2$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

Kaj pa se zgodi, če odstranimo $X_3$?

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=0.95, linetype="dashed")+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk"}
ggplot(data_sirina %>% filter(model %in%c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Pri modelih brez spremenljivke $X_3$ v primeru bolj simetrične porazdelitve $Gamma(5,5)$ opazimo

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm_x3 - pokritost1$ols_x3
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols_x3','glm_x3'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm_x3 - sirina1$ols_x3
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols_x3) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri modelu brez $X_3$ in $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk")
```

## Analiza variance in velikost učinka

Ker smo že v prejšnjih poglavjih ugotovili, da pri pokritosti in širini intervalov zaupanja ni večjih razlik med posameznimi regresijskimi koeficienti, bomo analizo variance naredili le na rezultatih za koeficient $\beta_1$. Na spodnjih grafih je tako prikazana velikost učinka pri analizi varianci za pokritost in kasneje še za širino intervala zaupanja glede na vključene spremenljivke. Rezultate iz grafov beremo hierarhično, torej koliko variabilnosti dodatno pojasni posamezna spremenljivka, glede na že upoštevane spremenljivke.

```{r  results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za pokritost intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 1))
simulacija$n <- as.factor(simulacija$n)
simulacija$korelacija <- as.factor(simulacija$korelacija)
simulacija$model <- as.factor(simulacija$model)
simulacija$alpha <- as.factor(simulacija$alpha)
model1 <- aov(pokritost_B1 ~ model * n * korelacija * alpha, data =simulacija)
plot(eta_squared(model1))
```

Iz zgornjega grafa lahko razberemo, da nobena od spremenljivk ne pojasni velikega deleža variabilnosti v pokritosti intervalov zaupanja. Vseeno pa je kar nekaj takih, ki pojasnijo manjši del in imajo statistično značilen vpliv (to nam pove povzetek modela anove, ki je v prilogi naloge).
Na začetku model pojasni $9\%$ variabilnosti pokritosti intervala zaupanja. Za tem velikost vzorca pojasni $4\%$ preostale variabilnosti in korelacija še dodatna $2\%$. Asimetričnost ostankov za tem dodatno ne pojasni nič variabilnosti. Nekoliko večji del variabilnosti (glede na ostale vplive) dodatno pojasnijo še interakcija med modelom in velikostjo vzorca ($10\%$), interakcija med modelom in korelacijo ($4\%$), interakcija med korelacijo in velikostjo vzorca ($2\%$) ter na koncu še interakcija modela, velikosti vzorca in korelacije ($4\%$).

Pri tem se moramo zavedati, da znotraj spremenljivke `model` nista samo \emph{glm} ali \emph{lm}, ampak so upoštevani še nepolni modeli. To poudarimo zato, ker je učinek najverjetneje posledica nepolnih modelov. Tudi v prejšnjih prikazih smo zaznali vpliv sledečih modelov, medtem ko med \emph{lm} in \emph{glm} modeloma nismo opazili razlik.
Vpliv variabilnosti in velikost vzorca smo prav tako zaznali že iz prejšnjih grafov, analiza variance in velikost učinka pa nam naša opažanja še dodatno potrdita. 
Poglejmo si še velikost učinka pri analizi variance za širino intervalov zaupanja.

```{r results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za širino intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 1))
model2 <- aov(sirina_B1 ~ n * korelacija * model *  alpha, data = as.data.frame(simulacija))
plot(eta_squared(model2))
```

Opazimo, da ni toliko spremenljivk, ki bi pojasnjevale variabilnost širine intervalov zaupanja, vendar le te pojasnijo večji del variabilnosti. $55\%$ variabilnost opazovane spremenljivke pojasni velikost vzorca, od preostale variabilnosti pa $16\%$ pojasni korelacija med neodvisnimi spremenljivkami. Model in asimetričnost ostankov dodatno ne pojasnita večjega dela variabilnosti širine intervala zaupanja, dodatnih $16\%$ pa pojasnjuje še interakcija med velikostjo vzorca in korelacijo. Manjši del variabilnosti dodatno pojasni še interakcija med velikostjo vzorca in modelom ($2\%$), ostale interakcije pa zanemarljivo majhne deleže.
Ponovno nam prikaz in analiza variance potrjujeta naša predhodna opazovanja. 

## Transformacija odzivne spremenljivke

V tem poglavju so predstavljeni še rezultati simulacij, kjer je uporabljena logaritemska tranformacija.
Pri teh simulacijah nas je zanimala predvsem razlika med uporabo transformacije $log(Y)$ v formuli funkcije \emph{lm} ($log(Y)=\beta X$) in uporabo logaritemske link funkcije ($link="log"$) v funkciji \emph{glm}. Vseeno pa si bomo pogledali tudi razlike med rezultati pri različnih parametrih.

```{r}
# Uvozimo še rezultate simulacij z logaritemsko transformacijo:
load("rezultati_simulacije_log.R")
for (j in c(2:5,9:15)){ # stolpce pretvorimo v numeric, ker so shranjeni kot character
  simulacija_log[,j] <- as.numeric(simulacija_log[,j])
}
for (j in c(6:8)){ # stolpce pretvorimo v factor, ker so shranjeni kot character
  simulacija_log[,j] <- as.logical(simulacija_log[,j])
  simulacija_log[,j] <- as.numeric(simulacija_log[,j])
}
simulacija_log$model <- as.factor(simulacija_log$model) # ime modela pretvorimo v tip factor
```

```{r}
# Shranimo posebej rezultate za pokritost in širine IZ
pokritost <- c("pokritost_B1", "pokritost_B2", "pokritost_B3")
sirina <- c()
data_pokritost <- melt(simulacija_log, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = pokritost)
data_pokritost$korelacija = data_pokritost$korelacija %>% round(1)
sirina <- c("sirina_B1", "sirina_B2", "sirina_B3")
data_sirina <- melt(simulacija_log, id.vars = c("model","korelacija", "alpha","alpha_X", "n"), measure.vars = sirina)
data_sirina$korelacija = data_sirina$korelacija %>% round(1)
```

### Zelo asimetrična porazdelitev pojasnjevalnih spremenljivk

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(2,5)$"}
ggplot(data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(2,5)$"}
ggplot(data_sirina %>% filter(model %in%c('ols','glm'), variable == 'sirina_B1', alpha_X==2), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Na Sliki 21 vidimo, da je pokritost IZ pri linearni regresiji boljša kot pri linearnih modelih. Razlika je očitna predvsem pri manjših vzorcih, pri dovolj velikih pa praktično ni razlik (številke so razvidne v tabeli 17).
Širina intervalov zaupanja je pri obeh metodah precej podobna, vendar se linearna regresija vseeno izkaže za malenkost boljšo. Razlike se povečujejo predvsem z večanjem korelacije med pojasnjevalnimi spremenljivkami in z večanjem vzorca (kar je bolje vidno v tabeli 18).
Predvidevamo, da je večanje širine intervalov zaupanja z večanjem stopnje asimetrije ostankov povezano s tem, da variance tokrat nismo skalirali.
Vseeno pa lahko rečemo, da se \emph{lm} model v primeru logaritemske transformacije pri $Gamma(2,5)$ porazdelitvi pojasnjevalnih spremenljivk obnese bolje od \emph{glm} modela tako pri pokritosti kot tudi širini IZ.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(2,5)$")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==2, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(2,5)$")
```


### Manj asimetrična porazdelitev pojasnjevalnih spremenljivk

```{r fig.cap="Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(5,5)$"}
ggplot(data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  geom_hline(yintercept=1)+ 
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "pokritost IZ")
```

```{r fig.cap="Širina intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(5,5)$"}
ggplot(data_sirina %>% filter(model %in%c('ols','glm'), variable == 'sirina_B1', alpha_X==5), aes(x = alpha, y = value, col = model)) +
  facet_grid(n~korelacija, labeller = label_both, scales = "free") +
  stat_summary(fun = mean, geom="line") +
  stat_summary(fun = mean, geom="point") +
  labs(x = expression(paste("Stopnja asimetrije ostankov (", alpha, ")", sep='')), y = "sirina IZ")
```

Pri $Gamma(5,5)$ porazdelitvi pojasnjevalnih spremenljivk pridemo do podobnih zaključkov kot pri $Gamma(2,5)$. Pokritost pri \emph{lm} je boljša od pokritosti pri \emph{glm} metodi, razlike so večje pri manjših vzorcih. Stopnja asimetrije ostankov tu ne igra vloge, prav tako korelacija med pojasnjevalnimi spremenljivkami.
Slika 24 je zelo podobna Sliki 22, le da je skala širine IZ pomaknjena nekoliko nižje. Predvsem pri manjših vzorcih je širina IZ v primeru porazdelitve $Gamma(2,5)$ v povprečju očitno večja od širine v primeru $Gamma(5,5)$. To opazimo, ko primerjamo tabeli 18 in 20.

```{r}
pokritost1 <- data_pokritost %>% filter(model %in% c('ols','glm'), variable == 'pokritost_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>% 
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
pokritost1$razlika <- pokritost1$glm - pokritost1$ols
kable(pokritost1, booktabs = F, align = "c", caption = "Pokritost intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(5,5)$")
```

```{r}
sirina1 <- data_sirina %>% filter(model %in% c('ols','glm'), variable == 'sirina_B1', alpha_X==5, n %in% c(10,100,1000), korelacija %in% c(0.3, 0.9)) %>%
  dcast(n+korelacija~model, value.var = 'value', fun.aggregate=mean) %>% round(2)
sirina1$razlika <- sirina1$glm - sirina1$ols
sirina1$razlika_pct <- (sirina1$razlika/sirina1$ols) %>% round(2)
kable(sirina1, booktabs = F, align = "c", caption = "Širina intervalov zaupanja za koeficient $\\beta_1$ pri $X_i \\sim Gamma(5,5)$")
```


### Analiza variance

Izvedemo podobno analizo kot pri modelih brez transformacije. Najprej si poglejmo analizo variance za pokritost, nato pa še za širino intervalov zaupanja.

```{r  results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za pokritost intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 3))
simulacija$n <- as.factor(as.character(simulacija_log$n))
simulacija$korelacija <- as.factor(as.character(simulacija_log$korelacija))
simulacija$model <- as.factor(as.character(simulacija_log$model))
simulacija$alpha <- as.factor(as.character(simulacija_log$alpha))
model3 <- aov(pokritost_B1 ~ model * n * korelacija * alpha, data = simulacija_log)
plot(eta_squared(model3))
```

Iz Slike 25 lahko razberemo, da nobena od spremenljivk ne pojasni večjega deleža variabilnosti pokritosti intervalov zaupanja. Vpliv modela, velikosti vzorca in interakcije med modelom in velikostjo vzorca imajo sicer statistično značilen vpliv (razvidno iz priloge), vendar vse spremenljivke skupaj pojasnijo manj kot $1\%$ variabilnosti.
Z danimi spremenljivkami pa lahko pojasnimo precej večji delež variabilnosti širine intervalov zaupanja, kar je razvidno na naslednji sliki.

```{r results=FALSE, fig.width=8, fig.cap="Velikost učinka pri analizi variance za širino intervala zaupanja"}
par(mar = c(5.1, 3.1, 4.1, 1))
model4 <- aov(sirina_B1 ~ n * korelacija *  alpha * model, data = as.data.frame(simulacija_log))
plot(eta_squared(model4))
```

Velikost vzorca pojasni $18\%$ variabilnosti, korelacija dodatno $7\%$ in zatem porazdelitev ostankov še dodatnih $5\%$. Pri porazdelitvi ostankov se moramo zavedati, da je to posledica različne variance, ne pa asimetrije porazdelitev. Model zatem dodatno pojasni zanemarljiv delež variabilnosti, interakcija med velikostjo vzorca in korelacijo dodatno $3\%$ in korelacija med velikostjo vzorca še $2\%$ variabilnosti širine intervalov zaupanja. Ostale interakcije dodatno pojasnijo zanemarljiv delež variabilnosti.

# Ugotovitve

Največji vpliv na ocene regresijskih koeficientov imata korelacija med pojasnjevalnimi spremenljivkami in velikost vzorca. 
Z večanjem korelacije se večajo širine intervalov zaupanja, kar je pričakovano, saj lahko del vpliva ene spremenljivke prevzame druga spremenljivka.
Z večanjem velikosti vzorca pri polnih modelih dobimo večjo pokritost pri \emph{glm} modelu in ožje intervale zaupanja pri obeh modelih (\emph{glm} in \emph{lm}).
Pri dovolj velikih vzorcih (tu izvzamemo $n=10$) ima \emph{glm} nekoliko boljšo pokritost, a širše intervale zaupanja, kot \emph{lm} model.
Pri najmanjšem opazovanem vzorcu ($n=10$) se pristop z \emph{lm} funkcijo izkaže za bolj učinkovitega pri pokritosti a slabšega pri širini intervalov zaupanja.

Pri večji asimetriji porazdelitve pojasnjevalnih spremenljivk dobimo nekoliko širše intervale zaupanja kot pri bolj simetrični porazdelitvi. Pri bolj simetrični porazdelitvi $Gamma(5,5)$ so tudi razlike med pristopoma z \emph{glm} in \emph{lm} manjše.Ob vsakem primeru pa velja, da se (procentualna) razlika v širini z večanjem velikosti vzorca povečuje v prid modela \emph{glm}.

Večja korelacija med pojasnjevalnimi spremenljivkami poveča širino intervalov zaupanja regresijskih koeficientov, na pokritosti pa nima večjega vpliva. Izjema so modeli brez spremenljivke $X_2$.
Asimetrija porazdelitve ostankov na ocene regresijskih koeficientov nima posebnega vpliva. V primeru logaritemske transformacije smo opazili, da se širina IZ z večanjem $\alpha$ (v $Gamma(\alpha, 5)$ porazdelitvi ostankov) povečuje, vendar pa to lahko pripišemo le vplivu večje variance in ne asimetrije.
Z večanjem velikosti vzorca se pričakovano ožajo intervali zaupanja regresijskih koeficientov. Na pokritost IZ velikost vzorca nima vpliva, razen v primeru modelov brez spremenljivke $X_2$.

Izločitev spremenljivke $X_3$ iz modela po pričakovanjih nima bistvenega vpliva na pokritost regresijskih koeficientov, saj smo podatke generirali pod predpostavko $\beta_3=0$. Vseeno pa v modelu brez $X_3$ dobimo nekoliko ožje intervale zaupanja, zaradi česar lahko tak model ocenimo kot najboljši (pravi). Rezultati so pričakovani, saj v modelih, kjer imamo vključene vse spremenljivke in neničelno korelacijo med pojasnjevalnimi spremenljivkami, del vpliva spremenljivke $X_2$ lahko prevzame spremenljivka $X_3$, zaradi česar prihaja do slabših rezultatov.
Izločitev spremenljivke $X_2$ predvsem pri večjih vzorcih in neničelni korelaciji med pojasnjevalnimi spremenljivkami vpliva na slabšo pokritost IZ regresijskega koeficienta $\beta_1$. V primeru ničelne korelacije in zadostne velikosti vzorca izločitev katerekoli od spremenljivk ($X_2$ ali $X_3$) iz modela ne vpliva na pokritost IZ regresijskega koeficienta $\beta_1$.

Nobena od spremenljivk, ki smo jih uporabili v tej nalogi, ne pojasni velikega deleža variabilnosti pokritosti intervalov zaupanja. Vseeno pa je nekaj takih, ki pojasnijo manjši delež variabilnosti in imajo statistično značilen vpliv. To so velikost vzorca, model (predvsem izključevanje spremenljivk), korelacija in različne interakcije.
Z manjšim številom spremenljivk pa lahko pojasnimo večji delež variabilnosti širine intervalov zaupanja. Kar $55\%$ variabilnosti pojasni velikost vzorca, večji delež preostale variabilnosti pa lahko pojasnimo s korelacijo in interakcijo med velikostjo vzorca in korelacijo.

Pri modelih z logaritemsko transformacijo se je pristop s funkcijo \emph{lm} izkazal za boljšega tako pri pokritosti kot širini intervalov zaupanja. Na tem področju bi bilo zanimivo preveriti še, kako se obnese \emph{glm} funkcija v primeru enake formule kot v \emph{lm} in zapisom $link="identity"$ (torej $log(Y)$ namesto $link="log"$). Poleg tega bi bilo še smiselno preveriti, če bi se $link$ funkcija morda bolje obnesla, če bi pri generiranju podatkov transformirali le ostanke, ne pa tudi pojasnjevalnih spremenljivk.

V modelih z logaritemsko transformacijo nobena od obravnavanih spremenljivk ne pojasni omembe vrednega deleža variabilnosti pokritosti IZ. Z določenimi spremenljivkami pa lahko pojasnimo večji delež variabilnosti širine IZ -  te spremenljivke so velikost vzorca, korelacija, porazdelitev ostankov (varianca ostankov) in nekatere interakcije.
Omeniti je še smiselno, da smo pri manj asimetrični porazdelitvi pojasnjevalnih spremenljivk $Gamma(5,5)$ dobili ožje intervale zaupanja regresijskih koeficientov.

Če pogledamo vse ugotovitve skupaj, lahko sklenemo, da če nimamo opravka s transformacijami, če imamo dovolj velik vzorec in če imamo gama porazdelitev ostankov, ima \emph{glm} model boljšo pokritost intervalov zaupanja, a na račun širših intervalov zaupanja. V primeru transformacije odzivne spremenljivke je bolj smiselno izbrati pristop, kjer že v vpisani formuli transformiramo odzivno spremenljivko, namesto da le to upoštevamo v $link$ parametru funkcije \emph{glm}. Seveda pa v praksi dobimo podatke, pri katerih sprva ne vemo, s kakšnimi transformacijami imamo opravka. V splošnem lahko rečemo, da je pristop z \emph{lm} modelom povsem "konkurenčen" pristopu z \emph{glm}, razlike v ocenah regresijskih koeficientov so relativno majhne. Delo P. E. Johnsona, ki je prišel do nekaterih podobnih ugotovitev, potrjuje naše sklepe. Sam je sklenil, da gama porazdelitev ostankov le redkokdaj vpliva na ocene regresijskih koeficientov, kljub temu, da so kršene nekatere predpostavke.

# Viri

- P. E. Johnson, \emph{GLM with a Gamma-distributed Dependent Variable}, [ogled 05.01.2020], dostopno na \url{https://pj.freefaculty.org/guides/stat/Regression-GLM/Gamma/GammaGLM-01.pdf?fbclid=IwAR14W34VhGzyG0wPiqNTk1hWjIToAug6a2TsPsTeZKLj_ntfTxaR1Aowiko}

- J. Jiang, \emph{Linear and Generalized Linear Mixed Models and Their Applications}, Springer Series in Statistics, Springer Science + Business Media, LLC, New York, 2007.

- V. Maver, \emph{Normalni linearni mešani modeli}, diplomsko delo, Fakulteta za matematiko in fiziko, Univerza v Ljubljani, 2018.

- \emph{glm}, v: RDocumentation, [ogled 30.12.2020], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/glm}.

- \emph{lm}, v: RDocumentation, [ogled 30.12.2020], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm}.

- \emph{confint}, v: RDocumentation, [ogled 02.01.2021], dostopno na \url{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/confint}

- M. Raič, \emph{O linearni regresijji}, 2014. Najdeno na spletnem naslovu:
\url{http://valjhun.fmf.uni-lj.si/~raicm/Odlomki/Linearna_regresija.pdf}

- L. Pfajfar, \emph{Osnovna ekonometrija}, učbeniki Ekonomske fakultete, Ljubljana, 2018.

- \emph{lcmix}, v: rdrr.io, [ogled 18.01.2021], dostopno na \url{https://rdrr.io/rforge/lcmix/src/R/distributions.R}

- \emph{mvgamma}, v: rdrr.io, [ogled 18.01.2021], dostopno na \url{https://rdrr.io/rforge/lcmix/man/mvgamma.html}

- K. Siegrist, \emph{5.8. The Gama Distribution}, [ogled 20.01.2021], dostopno na \url{https://stats.libretexts.org/Bookshelves/Probability_Theory/Book%3A_Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/05%3A_Special_Distributions/5.08%3A_The_Gamma_Distribution}

# Priloge

Koda za simulacije brez transformacij se nahaja v priloženi datoteki `Simulacije.R`, njeni rezultati pa v datoteki `rezultati_simulacije.R`.
Koda za simulacije s transformacijami se nahaja v priloženi datoteki `Simulacije_log.R`, njeni rezultati pa v datoteki `rezultati_simulacije_log.R`.
Poročilo je bilo pripravljeno v `seminarska_RZM.Rmd` datoteki, ki vsebuje tudi kodo za grafe. Za pravilen prevod Rmd datoteke je potrebno v isti mapi imeti datoteko `header.tex`, ki poskrbi za lepši prevod datoteke (slovenski naslovi slik, tabel in podobno).

Sledi izpis ANOVE za pokritost intervalov zaupanja pri modelu brez transformacije.

```{r}
summary(model1)
```

Sledi izpis ANOVE za širino intervalov zaupanja pri modelu brez transformacije.

```{r}
summary(model2)
```

Sledi izpis ANOVE za pokritost intervalov zaupanja pri modelu s transformacijo.

```{r}
summary(model3)
```

Sledi izpis ANOVE za širino intervalov zaupanja pri modelu s transformacijo.

```{r}
summary(model4)
```

